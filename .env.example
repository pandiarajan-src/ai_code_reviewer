# Bitbucket Configuration
BITBUCKET_URL=https://your-bitbucket-server.com
BITBUCKET_TOKEN=your_bitbucket_access_token

# LLM Configuration
LLM_PROVIDER=openai
LLM_API_KEY=your_openai_api_key
LLM_ENDPOINT=https://api.openai.com/v1/chat/completions
LLM_MODEL=gpt-4o

# Ollama Configuration (for local LLM)
OLLAMA_HOST=http://localhost:11434

# Security
WEBHOOK_SECRET=your_webhook_secret

# Email Configuration
LOGIC_APP_EMAIL_URL=https://your-logic-app-url
LOGIC_APP_FROM_EMAIL=pandiarajans@test.com
EMAIL_OPTOUT=true

# Server Configuration
HOST=0.0.0.0
BACKEND_PORT=8000
FRONTEND_PORT=3000
LOG_LEVEL=INFO

# Database Configuration
# Production (Docker): Use absolute path to mounted volume
# Development (Local): Use relative path to workspace
# Docker default: sqlite+aiosqlite:////app/data/ai_code_reviewer.db
# Local default: sqlite+aiosqlite:///./ai_code_reviewer.db
# Development Docker: sqlite+aiosqlite:////app/data/ai_code_reviewer_dev.db
DATABASE_URL=sqlite+aiosqlite:///./ai_code_reviewer.db
DATABASE_ECHO=false

# Coding Guidelines Configuration
# Path to the coding guidelines markdown file
# Default: Guidelines/Universal_Engineering_Coding_Guidelines.md (relative to project root)
# Set to custom path if you have your own guidelines file
GUIDELINES_FILE=Guidelines/Universal_Engineering_Coding_Guidelines.md
# Enable/disable guidelines-based code review (true/false)
# When enabled, the LLM will check code against the specified guidelines
# and flag violations of "Must Comply" rules
GUIDELINES_ENABLED=true
